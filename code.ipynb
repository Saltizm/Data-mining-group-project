{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe9zoZNbtmad+1gm39Sl0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saltizm/Data-mining-group-project/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrV0dHQIxMci"
      },
      "outputs": [],
      "source": [
        "import sklearn as sk;\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "import seaborn as sns;\n",
        "import matplotlib.pyplot as plt;\n",
        "import tqdm;\n",
        "import os;\n",
        "import sys;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # mount\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kTD1xFY744wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(r'C:\\Users\\User\\Desktop\\Data-mining-group-project'):\n",
        "    os.chdir(r'C:\\Users\\User\\Desktop\\Data-mining-group-project') #local\n",
        "elif os.path.isdir(r'/content/drive/MyDrive/data mining'):\n",
        "    os.chdir(r'/content/drive/MyDrive/data mining') #google drive\n",
        "try:\n",
        "    train = pd.read_csv(open('UNSW_NB15_training-set.csv', encoding='utf-8'))\n",
        "    test = pd.read_csv(open('UNSW_NB15_testing-set.csv', encoding='utf-8'))\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"error: {e}\\nTry changing the training data directory in 'os.chdir'\")"
      ],
      "metadata": {
        "id": "9YhNscWR21A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "OUTLIER_THRESHOLD = 0.1\n",
        "FEATURE_THRESHOLD = 0.5\n",
        "BIN_THRESHOLD = 0.1"
      ],
      "metadata": {
        "id": "zXeyOYVSlmnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "vAc-dS0slpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if data too big (use for testing)\n",
        "train = train.sample(frac=0.1)\n",
        "test = test.sample(frac=0.1)"
      ],
      "metadata": {
        "id": "-sBRf60_NIGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape, test.shape)\n",
        "# 45 attributes"
      ],
      "metadata": {
        "id": "kc1xLoiwYftn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().any(axis=1).sum()"
      ],
      "metadata": {
        "id": "cSDW-mZC5NGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.isnull().any(axis=1).sum()"
      ],
      "metadata": {
        "id": "OoIoHadI5S64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import files\n",
        "train = train.sample(frac=1)\n",
        "test = test.sample(frac=1)\n",
        "x_train, y_train = train.iloc[:, :-2], train.iloc[:, -2:]\n",
        "x_test, y_test = test.iloc[:, :-2], test.iloc[:, -2:]"
      ],
      "metadata": {
        "id": "oVM-kh1R4-gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.columns"
      ],
      "metadata": {
        "id": "0uAGkw2emft-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183eb958"
      },
      "source": [
        "def show_outliers_iqr(series):\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return (lower_bound, upper_bound)\n",
        "\n",
        "# data cleaning (replacement)\n",
        "def replace_outliers_iqr(df, cols):\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            lower_bound, upper_bound = show_outliers_iqr(df[col])\n",
        "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return df\n",
        "\n",
        "# data filtering\n",
        "def remove_outliers_iqr(df, cols):\n",
        "    overall_mask = pd.Series(True, index=df.index)\n",
        "\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            lower_bound, upper_bound = show_outliers_iqr(df[col])\n",
        "            col_mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)\n",
        "            overall_mask = overall_mask & col_mask\n",
        "\n",
        "    df = df[overall_mask]\n",
        "    return df\n",
        "\n",
        "def risky_show_shape(*arg):\n",
        "    if len(arg) == 2:\n",
        "        print(\"train/test\")\n",
        "        print(arg[0].shape, arg[1].shape)\n",
        "    else:\n",
        "        print(\"x_train/x_test/y_train/y_test\")\n",
        "        for df in arg:\n",
        "            print(df.shape)\n",
        "\n",
        "def run_model(model, x_train, y_train, x_test, y_test):\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    return y_pred\n",
        "\n",
        "def plot_histograms(df, numerical_cols, frac=0.1):\n",
        "    df = df.copy().sample(frac=frac)\n",
        "    # Filter numerical_cols to only include those present in binned_x_train\n",
        "    columns_to_plot = [col for col in numerical_cols if col in df.columns]\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    num_plots = len(columns_to_plot)\n",
        "    num_cols = 4\n",
        "    num_rows = (num_plots + num_cols - 1) // num_cols # Equivalent to ceil(num_plots / num_cols)\n",
        "\n",
        "    plt.figure(figsize=(20, num_rows * 5)) # Adjust figure size for better readability\n",
        "\n",
        "    for i, col in enumerate(columns_to_plot):\n",
        "        ax = plt.subplot(num_rows, num_cols, i + 1) # Create subplot\n",
        "        sns.histplot(df[col], kde=True, ax=ax)\n",
        "        ax.set_title(f'Histogram of {col}')\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Frequency')\n",
        "\n",
        "    # Hide any unused subplots if the last row is not full\n",
        "    for j in range(i + 1, num_rows * num_cols):\n",
        "        plt.subplot(num_rows, num_cols, j + 1).set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# def bin_decision(unique_series, threshold=0.2):\n",
        "#     keep = []\n",
        "\n",
        "#     val_count = unique_series.value_counts()\n",
        "#     for col in val_count.index:\n",
        "#         if val_count[col] / unique_series.shape[0] > threshold:\n",
        "#             keep.append(col)\n",
        "#     return keep\n",
        "\n",
        "# def binning(df, col, values):\n",
        "#     df = df.copy()\n",
        "\n",
        "#     for val in values:\n",
        "#         df[col].replace(val, 'Others', inplace=True)\n",
        "#     return df\n",
        "\n",
        "class Plot(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, frac=0.1):\n",
        "        self.frac = frac\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        plot_histograms(X, X.columns, self.frac)\n",
        "        return X\n",
        "\n",
        "class Add_feature(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_1, feature_2):\n",
        "        self.feature_1 = feature_1\n",
        "        self.feature_2 = feature_2\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['new_feature'] = X[self.feature_1] + X[self.feature_2]\n",
        "        return X\n",
        "\n",
        "class Ratio_feature(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_1, feature_2):\n",
        "        self.feature_1 = feature_1\n",
        "        self.feature_2 = feature_2\n",
        "        if self.feature_2 == 0:\n",
        "            self.feature_2 = 1\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['new_feature'] = X[self.feature_1] / X[self.feature_2]\n",
        "        return X\n",
        "\n",
        "class capping(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return replace_outliers_iqr(X, self.cols)\n",
        "\n",
        "class filtering(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return remove_outliers_iqr(X, self.cols)\n",
        "\n",
        "class Threshold_binning(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.2, other_label=\"Others\"):\n",
        "        self.threshold = threshold\n",
        "        self.other_label = other_label\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        self.keep_ = {}\n",
        "\n",
        "        for col in X.columns:\n",
        "            freq = X[col].value_counts(normalize=True)\n",
        "            self.keep_[col] = freq[freq > self.threshold].index\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            keep = self.keep_.get(col, [])\n",
        "            X[col] = X[col].where(X[col].isin(keep), self.other_label)\n",
        "\n",
        "        return X\n",
        "\n",
        "class Pearson_feature_selection(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Calculate correlation between each feature in X and target y\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            correlations = X.corrwith(y).abs()\n",
        "        else:\n",
        "            # Handle numpy arrays\n",
        "            X_df = pd.DataFrame(X)\n",
        "            y_series = pd.Series(y)\n",
        "            correlations = X_df.corrwith(y_series).abs()\n",
        "\n",
        "        # Keep features with correlation above threshold\n",
        "        self.keep_ = correlations > self.threshold\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X.loc[:, self.keep_]\n",
        "        else:\n",
        "            # Handle numpy arrays\n",
        "            return X[:, self.keep_.values]\n",
        "\n",
        "message = \"\"\"\n",
        "plot_histograms(frac=0.1) -\n",
        "plot a sampled subset of the data.\n",
        "\n",
        "show_outliers_iqr(series) -\n",
        "return an array where index=0 is the lower bound and index=1 is the upper bound.\n",
        "\n",
        "replace_outliers_iqr(df, cols) -\n",
        "creates a copy of \"df\". replace the samples with attributes > upper_bound or < lower_bound with upper_bound or lower_bound.\n",
        "\n",
        "remove_outliers_iqr(df, cols) -\n",
        "creates a copy of \"df\". remove the samples with attributes > upper_bound or < lower_bound.\n",
        "\n",
        "risky_show_shape(*data) -\n",
        "in the format of train/test. Or x_train, x_test, y_train, y_test.\n",
        "\n",
        "run_model(model, x_train, y_train, x_test, y_test) -\n",
        "runs the model and returns the prediction.\n",
        "\n",
        "bin_decision(unique_series, threshold=0.2) -\n",
        "receives a series df['col'].\n",
        "get a list of value that represent less than x% of the total.\n",
        "\n",
        "binning(df, col, values) -\n",
        "replace a list of values with 'others' for one hot encoding later on.\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "User defined Transformer;\n",
        "\n",
        "Plot(BaseEstimator, TransformerMixin) -\n",
        "wrapper function for plot_histograms\n",
        "\n",
        "Add_feature(BaseEstimator, TransformerMixin) -\n",
        "Transformer that adds two numerical features to create a new secondary feature.\n",
        "\n",
        "Ratio_feature(BaseEstimator, TransformerMixin) -\n",
        "Transformer that divide two numerical features to create a new secondary feature.\n",
        "if denominator value=0. reassign it to 1.\n",
        "\n",
        "Capping/Filtering(BaseEstimator, TransformerMixin) -\n",
        "wrapper function for replace_outliers_iqr/remove_outliers_iqr\n",
        "\n",
        "Threshold_binning(BaseEstimator, TransformerMixin) -\n",
        "bins values below a threshold into 'others'. Replacement for defunct binning and bin decision function rip\n",
        "\n",
        "Pearson_feature_selection(BaseEstimator, TransformerMixin) -\n",
        "select features based on corr coe\n",
        "\"\"\"\n",
        "print(message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find numerical and non numerical fields\n",
        "numerical_cols = x_train.select_dtypes(include=np.number).columns\n",
        "non_numerical_cols = x_train.select_dtypes(exclude=np.number).columns"
      ],
      "metadata": {
        "id": "CTiMYNNGxtl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find continuous and uniary fields (low and high cardinality)\n",
        "\n",
        "low_cardinality_cols = []\n",
        "high_cardinality_cols = []\n",
        "continuous_cols = []\n",
        "\n",
        "# Define a threshold for low cardinality. For example, less than 20 unique values.\n",
        "CARDINALITY_THRESHOLD = 50\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if col in ['id', 'label']:\n",
        "        continue\n",
        "\n",
        "    unique_count = x_train[col].nunique()\n",
        "    if unique_count <= CARDINALITY_THRESHOLD:\n",
        "        low_cardinality_cols.append(col)\n",
        "    else:\n",
        "        continuous_cols.append(col)\n",
        "\n",
        "# Non-numerical columns can also be high cardinality if they have many unique categorical values\n",
        "# This step categorizes based on numerical data first.\n",
        "# Let's consider non-numerical columns for high cardinality if they have many unique values\n",
        "for col in non_numerical_cols:\n",
        "    if col in ['id', 'label']:\n",
        "        continue\n",
        "    unique_count = x_train[col].nunique()\n",
        "    if unique_count > CARDINALITY_THRESHOLD:\n",
        "        high_cardinality_cols.append(col)\n",
        "    else:\n",
        "        low_cardinality_cols.append(col)"
      ],
      "metadata": {
        "id": "wbXgI_Rfr5zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Continuous Columns ({len(continuous_cols)}): {continuous_cols}\")\n",
        "print(f\"Low Cardinality Columns ({len(low_cardinality_cols)}): {low_cardinality_cols}\")\n",
        "print(f\"High Cardinality (Non-numerical) Columns ({len(high_cardinality_cols)}): {high_cardinality_cols}\")"
      ],
      "metadata": {
        "id": "__Z276OLxahQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d55366"
      },
      "source": [
        "### Histograms for Continuous Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "030b7ef3"
      },
      "source": [
        "plot_histograms(x_train, continuous_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c699b52f"
      },
      "source": [
        "### Histograms for Low Cardinality Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ec335f"
      },
      "source": [
        "plot_histograms(x_train, low_cardinality_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64158d04"
      },
      "source": [
        "### Histograms for High Cardinality Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be70bbde"
      },
      "source": [
        "plot_histograms(x_train, high_cardinality_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if there are any outliers\n",
        "outlier_data = []\n",
        "\n",
        "for col in continuous_cols:\n",
        "    lower_bound, upper_bound = show_outliers_iqr(train[col])\n",
        "    lower_outliers_count = (x_train[col] < lower_bound).sum()\n",
        "    upper_outliers_count = (x_train[col] > upper_bound).sum()\n",
        "    non_outliers_count = len(x_train[col]) - (lower_outliers_count + upper_outliers_count)\n",
        "\n",
        "    outlier_data.append({\n",
        "        'Column': col,\n",
        "        'Lower Bound Outlier %': f\"{lower_outliers_count/len(x_train[col]):.2f}\",\n",
        "        'Upper Bound Outlier %': f\"{upper_outliers_count/len(x_train[col]):.2f}\",\n",
        "        'Non-Outlier %': f\"{non_outliers_count/len(x_train[col]):.2f}\"\n",
        "    })\n",
        "\n",
        "# significant outliers\n",
        "outlier_df = pd.DataFrame(outlier_data)\n",
        "outlier_df_high = outlier_df[outlier_df['Upper Bound Outlier %'] > f'{OUTLIER_THRESHOLD}']\n",
        "outlier_df_low = outlier_df[outlier_df['Upper Bound Outlier %'] < f'{OUTLIER_THRESHOLD}']\n",
        "non_outlier_df = outlier_df[outlier_df['Upper Bound Outlier %'] == '0.00']\n",
        "# outlier_df"
      ],
      "metadata": {
        "id": "NeG912R_5ksV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_df"
      ],
      "metadata": {
        "id": "ztZMH4HQiegE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('non_outliers/ outlier_high /outlier_low')\n",
        "print(non_outlier_df.shape, outlier_df_high.shape, outlier_df_low.shape)"
      ],
      "metadata": {
        "id": "_s6KylLKzhPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KN5EkVF4EoPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns before calculating correlation\n",
        "corr_matrix = train.select_dtypes(include=[np.number]).corr()\n",
        "\n",
        "# Get correlations with the 'label' column\n",
        "label_correlations = corr_matrix['label'].drop('label').drop('id') # Exclude self-correlation\n",
        "\n",
        "# Find the feature with the highest absolute correlation\n",
        "top5_features = label_correlations.abs().nlargest(5)\n",
        "highest_corr_value = top5_features.max()\n",
        "print(f\"Feature with highest absolute correlation to 'label': \\n{top5_features}\")\n",
        "print(f\"Highest correlation coefficient: {highest_corr_value:.4f}\")\n",
        "\n",
        "# 3. Plot the heatmap\n",
        "plt.figure(figsize=(28, 18)) # Adjust figure size if needed\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZU5Mxv1QoGbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "pairs = combinations(top5_features.index, 2)\n",
        "\n",
        "# Initialize temp with numerical columns from train. Correcting the TypeError here.\n",
        "temp = train.select_dtypes(include=[np.number]).copy() # Use .copy() to avoid SettingWithCopyWarning later if modifying\n",
        "\n",
        "for pair in pairs:\n",
        "    feature1, feature2 = pair\n",
        "\n",
        "    add_trans = Add_feature(feature1, feature2)\n",
        "    ratio_trans = Ratio_feature(feature1, feature2)\n",
        "\n",
        "    # Apply Add_feature and rename the new column to be unique\n",
        "    transformed_add = add_trans.fit_transform(temp.copy()) # Use a copy to prevent in-place modification within transformer's fit_transform\n",
        "    add_feature_name = f'{feature1}_{feature2}_add'\n",
        "    temp[add_feature_name] = transformed_add['new_feature']\n",
        "\n",
        "    # Apply Ratio_feature and rename the new column to be unique\n",
        "    transformed_ratio = ratio_trans.fit_transform(temp.copy()) # Use a copy\n",
        "    ratio_feature_name = f'{feature1}_{feature2}_ratio'\n",
        "    temp[ratio_feature_name] = transformed_ratio['new_feature']\n",
        "\n",
        "# After generating all new features, drop the original top5_features columns\n",
        "# Correcting the inplace=True issue: assign the result back to temp.\n",
        "# Use list() for columns and errors='ignore' to handle cases where top5_features.index might contain columns not in temp.\n",
        "temp = temp.drop(columns=numerical_cols[:-1], errors='ignore')\n",
        "\n",
        "# plot a coefficient matrix for temp\n",
        "temp_corr_matrix = temp.corr()\n",
        "\n",
        "# 3. Plot the heatmap\n",
        "label_correlations = temp_corr_matrix['label'].drop('label') # Exclude self-correlation\n",
        "top5_features = label_correlations.abs().nlargest(5)\n",
        "highest_corr_value = top5_features.max()\n",
        "print(f\"Feature with highest absolute correlation to 'label': \\n{top5_features}\")\n",
        "print(f\"Highest correlation coefficient: {highest_corr_value:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(28, 18)) # Adjust figure size if needed\n",
        "sns.heatmap(temp_corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SuvIiIdIFKlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# highest correlation feature is sttl_ct_state_ttl_add (ttl_sum)\n",
        "column_modifier = Pipeline([\n",
        "    (\"ttl_sum\", Add_feature('sttl', 'ct_state_ttl')),\n",
        "    (\"drop_original\", ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"drop_columns\", \"drop\", [\"sttl\", \"ct_state_ttl\"])\n",
        "        ], remainder=\"passthrough\"\n",
        "    )),\n",
        "    (\"pearson_selection\", Pearson_feature_selection(threshold=FEATURE_THRESHOLD))\n",
        "])\n",
        "\n",
        "# forming pipeline\n",
        "preprocessing = Pipeline([\n",
        "    (\"capping\", capping(high_cardinality_cols)),\n",
        "    (\"filtering\", filtering(high_cardinality_cols)),\n",
        "    (\"logrithmic_transform\", FunctionTransformer(np.log1p)),\n",
        "    (\"bin_rare\", Threshold_binning(threshold=BIN_THRESHOLD)),\n",
        "    (\"modify_columns\", column_modifier),\n",
        "    (\"ordinal\", OrdinalEncoder(\n",
        "        handle_unknown=\"use_encoded_value\",\n",
        "        unknown_value=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# run pipeline\n",
        "encoded_x_train = preprocessing.fit_transform(x_train, y_train)\n",
        "encoded_x_test = preprocessing.transform(x_test)"
      ],
      "metadata": {
        "id": "4aOpS6MuAYQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6febb0f0"
      },
      "source": [
        "# !pip install xgboost #uncomment if error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "hyperparameters = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d45ec3b"
      },
      "source": [
        "# Random Forest classifier\n",
        "param_grid = {\n",
        "    'max_depth': [10,30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False],\n",
        "\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid=param_grid, cv=cv_strategy)\n",
        "grid_search.fit(encoded_x_train, y_train)\n",
        "hyperparameters['Random_Forest'] = grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "aUIQvxySMtRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression classifier\n",
        "param_grid_lr = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'max_iter': [100, 200]\n",
        "}\n",
        "\n",
        "grid_search_lr = GridSearchCV(LogisticRegression(random_state=42), param_grid=param_grid_lr, cv=cv_strategy)\n",
        "grid_search_lr.fit(encoded_x_train, y_train)\n",
        "hyperparameters['Logistic_Regression'] = grid_search_lr.best_estimator_\n",
        "\n",
        "print(\"Logistic Regression best parameters:\")\n",
        "print(grid_search_lr.best_params_)"
      ],
      "metadata": {
        "id": "ukAsgK1rP2pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.7, 1.0],\n",
        "    'colsample_bytree': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "grid_search_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid=param_grid_xgb, cv=cv_strategy, verbose=1, n_jobs=-1)\n",
        "grid_search_xgb.fit(encoded_x_train, y_train)\n",
        "hyperparameters['XGBoost'] = grid_search_xgb.best_estimator_\n",
        "\n",
        "print(\"XGBoost best parameters:\")\n",
        "print(grid_search_xgb.best_params_)"
      ],
      "metadata": {
        "id": "nMpEsXl9QOtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "for model_name, model_estimator in hyperparameters.items():\n",
        "    print(f\"Generating Confusion Matrix for {model_name}\")\n",
        "\n",
        "    # Make predictions on the encoded test set\n",
        "    y_pred = model_estimator.predict(encoded_x_test)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sIa0gTezQV7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for model_name, model_estimator in hyperparameters.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "\n",
        "    # Make predictions on the encoded test set\n",
        "    y_pred = model_estimator.predict(encoded_x_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(results_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "Ul6eJGzdQt8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}