{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7WhTFGQbYYYu0Va5hQtmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saltizm/Data-mining-group-project/blob/main/pyspark_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BrV0dHQIxMci"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "import pyspark as s\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "import seaborn as sns;\n",
        "import matplotlib.pyplot as plt;\n",
        "import tqdm;\n",
        "import os;\n",
        "import sys;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kTD1xFY744wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21833624-8261-4514-f6cd-dcb0582c0fb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "811d728e"
      },
      "source": [
        "# Initialize SparkSession if not already initialized\n",
        "spark = SparkSession.builder.appName(\"pyspark_code\").getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isdir(r'C:\\Users\\User\\Desktop\\Data-mining-group-project'):\n",
        "    fpath = r'C:\\Users\\User\\Desktop\\Data-mining-group-project' #local\n",
        "elif os.path.isdir(r'/content/drive/MyDrive/data mining'):\n",
        "    fpath = r'/content/drive/MyDrive/data mining' #google drive\n",
        "try:\n",
        "    print(os.getcwd())\n",
        "    train = spark.read.csv(fpath + os.sep + r'UNSW_NB15_training-set.csv')\n",
        "    test = spark.read.csv(fpath + os.sep + r'UNSW_NB15_testing-set.csv')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"error: {e}\\nTry changing the training data directory in 'os.chdir'\")"
      ],
      "metadata": {
        "id": "9YhNscWR21A1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8f2c6a-04f5-40a0-ced2-efa351afff26"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data mining\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if data too big (use for testing)\n",
        "train = train.sample(frac=0.1, random_state=42)\n",
        "test = test.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "-sBRf60_NIGF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all column names from the training Spark DataFrame\n",
        "cols = train.columns\n",
        "\n",
        "# Identify feature columns (all but the last two)\n",
        "x_cols = cols[:-2]\n",
        "\n",
        "# Identify the label column (the last one)\n",
        "y_cols = cols[-1:]\n",
        "\n",
        "# Select columns to create x_train_spark and y_train_spark\n",
        "x_train_spark = train.select(*x_train_cols)\n",
        "y_train_spark = train.select(y_train_col).withColumnRenamed(y_train_col, 'label')\n",
        "\n",
        "# Repeat for the test set\n",
        "all_test_cols = test.columns\n",
        "x_test_cols = all_test_cols[:-2]\n",
        "y_test_col = all_test_cols[-1]\n",
        "\n",
        "x_test = test.select(*x_test_cols)\n",
        "y_test = test.select(y_test_col).withColumnRenamed(y_test_col, 'label')"
      ],
      "metadata": {
        "id": "ABTqjDKRbZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape, test.shape)\n",
        "# 45 attributes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc1xLoiwYftn",
        "outputId": "11e6ae3f-1f70-4e54-b404-62d208a9849b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17534, 45) (8233, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().any(axis=1).sum()"
      ],
      "metadata": {
        "id": "cSDW-mZC5NGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2403def-30d9-440f-929b-0421c9841efd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.isnull().any(axis=1).sum()"
      ],
      "metadata": {
        "id": "OoIoHadI5S64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8473af-8663-4ca8-8a5a-e65b568dc5f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import files\n",
        "x_train, y_train = train.iloc[:, :-2].sample(frac=1, random_state=42), train.iloc[:, -1:].sample(frac=1, random_state=42)\n",
        "x_test, y_test = test.iloc[:, :-2].sample(frac=1, random_state=42), test.iloc[:, -1:].sample(frac=1, random_state=42)"
      ],
      "metadata": {
        "id": "oVM-kh1R4-gi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183eb958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a472201-9ce9-4e5f-872a-30fe6cd6b6d9"
      },
      "source": [
        "def show_outliers_iqr(series):\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return (lower_bound, upper_bound)\n",
        "\n",
        "# data cleaning (replacement)\n",
        "def replace_outliers_iqr(df, cols):\n",
        "    df = df.copy()\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            lower_bound, upper_bound = show_outliers_iqr(df[col])\n",
        "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return df\n",
        "\n",
        "# data filtering\n",
        "def remove_outliers_iqr(df, cols):\n",
        "    df = df.copy()\n",
        "    overall_mask = pd.Series(True, index=df.index)\n",
        "\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            lower_bound, upper_bound = show_outliers_iqr(df[col])\n",
        "            col_mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)\n",
        "            overall_mask = overall_mask & col_mask\n",
        "\n",
        "    df = df[overall_mask]\n",
        "    return df\n",
        "\n",
        "def risky_show_shape(*arg):\n",
        "    if len(arg) == 2:\n",
        "        print(\"train/test\")\n",
        "        print(arg[0].shape, arg[1].shape)\n",
        "    else:\n",
        "        print(\"x_train/x_test/y_train/y_test\")\n",
        "        for df in args:\n",
        "            print(df.shape)\n",
        "\n",
        "def run_model(model, x_train, y_train, x_test, y_test):\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    return y_pred\n",
        "\n",
        "def bin_decision(series, threshold=0.2):\n",
        "    keep = []\n",
        "\n",
        "    val_count = series.value_counts()\n",
        "    for col in val_count.index:\n",
        "        if val_count[col] / series.shape[0] > threshold:\n",
        "            keep.append(col)\n",
        "    return keep\n",
        "\n",
        "def binning(df, col, values):\n",
        "    df = df.copy()\n",
        "\n",
        "    for val in values:\n",
        "        df[col].replace(val, 'Others')\n",
        "    return df\n",
        "\n",
        "def col_encoder(df, col):\n",
        "    df = df.copy()\n",
        "\n",
        "    encoded_col = pd.get_dummies(df[col], prefix=col)\n",
        "    df_encoded = pd.concat([df.drop(col, axis=1), encoded_col], axis=1)\n",
        "    return df_encoded\n",
        "\n",
        "message = \"\"\"\n",
        "show_outliers_iqr(series) -\n",
        "return an array where index=0 is the lower bound and index=1 is the upper bound.\n",
        "\n",
        "replace_outliers_iqr(df, cols) -\n",
        "creates a copy of \"df\". replace the samples with attributes > upper_bound or < lower_bound with upper_bound or lower_bound.\n",
        "\n",
        "remove_outliers_iqr(df, cols) -\n",
        "creates a copy of \"df\". remove the samples with attributes > upper_bound or < lower_bound.\n",
        "\n",
        "risky_show_shape(*data) -\n",
        "in the format of train/test. Or x_train, x_test, y_train, y_test.\n",
        "\n",
        "run_model(model, x_train, y_train, x_test, y_test) -\n",
        "runs the model and returns the prediction.\n",
        "\n",
        "bin_decision(series, threshold=0.2) -\n",
        "get a list of value that represent less than x% of the total.\n",
        "\n",
        "binning(df, col, values) -\n",
        "replace a list of values with 'others' for one hot encoding later on.\n",
        "\"\"\"\n",
        "print(message)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "show_outliers_iqr(series) -\n",
            "return an array where index=0 is the lower bound and index=1 is the upper bound.\n",
            "\n",
            "replace_outliers_iqr(df, cols) -\n",
            "creates a copy of \"df\". replace the samples with attributes > upper_bound or < lower_bound with upper_bound or lower_bound.\n",
            "\n",
            "remove_outliers_iqr(df, cols) -\n",
            "creates a copy of \"df\". remove the samples with attributes > upper_bound or < lower_bound.\n",
            "\n",
            "risky_show_shape(*data) -\n",
            "in the format of train/test. Or x_train, x_test, y_train, y_test.\n",
            "\n",
            "run_model(model, x_train, y_train, x_test, y_test) -\n",
            "runs the model and returns the prediction.\n",
            "\n",
            "bin_decision(series, threshold=0.2) -\n",
            "get a list of value that represent less than x% of the total.\n",
            "\n",
            "binning(df, col, values) -\n",
            "replace a list of values with 'others' for one hot encoding later on.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find numerical and non numerical fields\n",
        "numerical_cols = x_train.select_dtypes(include=np.number).columns\n",
        "non_numerical_cols = x_train.select_dtypes(exclude=np.number).columns"
      ],
      "metadata": {
        "id": "CTiMYNNGxtl1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find continuous and uniary fields (low and high cardinality)\n",
        "\n",
        "low_cardinality_cols = []\n",
        "high_cardinality_cols = []\n",
        "continuous_cols = []\n",
        "\n",
        "# Define a threshold for low cardinality. For example, less than 20 unique values.\n",
        "CARDINALITY_THRESHOLD = 50\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if col in ['id', 'label']:\n",
        "        continue\n",
        "\n",
        "    unique_count = x_train[col].nunique()\n",
        "    if unique_count <= CARDINALITY_THRESHOLD:\n",
        "        low_cardinality_cols.append(col)\n",
        "    else:\n",
        "        continuous_cols.append(col)\n",
        "\n",
        "# Non-numerical columns can also be high cardinality if they have many unique categorical values\n",
        "# This step categorizes based on numerical data first.\n",
        "# Let's consider non-numerical columns for high cardinality if they have many unique values\n",
        "for col in non_numerical_cols:\n",
        "    if col in ['id', 'label']:\n",
        "        continue\n",
        "    unique_count = x_train[col].nunique()\n",
        "    if unique_count > CARDINALITY_THRESHOLD:\n",
        "        high_cardinality_cols.append(col)\n",
        "    else:\n",
        "        low_cardinality_cols.append(col)"
      ],
      "metadata": {
        "id": "wbXgI_Rfr5zN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Continuous Columns ({len(continuous_cols)}): {continuous_cols}\")\n",
        "print(f\"Low Cardinality Columns ({len(low_cardinality_cols)}): {low_cardinality_cols}\")\n",
        "print(f\"High Cardinality (Non-numerical) Columns ({len(high_cardinality_cols)}): {high_cardinality_cols}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__Z276OLxahQ",
        "outputId": "33310cb3-671e-4cdd-b366-b2e98c73598e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuous Columns (26): ['ï»¿id', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'response_body_len', 'ct_srv_src', 'ct_dst_src_ltm', 'ct_srv_dst']\n",
            "Low Cardinality Columns (16): ['sttl', 'dttl', 'swin', 'dwin', 'trans_depth', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'is_sm_ips_ports', 'service', 'state']\n",
            "High Cardinality (Non-numerical) Columns (1): ['proto']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d55366"
      },
      "source": [
        "### Histograms for Continuous Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "030b7ef3"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Calculate grid dimensions\n",
        "# num_plots = len(continuous_cols)\n",
        "# num_rows = (num_plots + 3) // 4 # Equivalent to ceil(num_plots / 4)\n",
        "# num_cols = 4\n",
        "\n",
        "# plt.figure(figsize=(20, num_rows * 5)) # Adjust figure size for better readability\n",
        "\n",
        "# for i, col in enumerate(continuous_cols):\n",
        "#     ax = plt.subplot(num_rows, num_cols, i + 1) # Create subplot\n",
        "#     sns.histplot(x_train[col], kde=True, ax=ax)\n",
        "#     ax.set_title(f'Histogram of {col}')\n",
        "#     ax.set_xlabel(col)\n",
        "#     ax.set_ylabel('Frequency')\n",
        "\n",
        "# # Hide any unused subplots if the last row is not full\n",
        "# for j in range(i + 1, num_rows * num_cols):\n",
        "#     plt.subplot(num_rows, num_cols, j + 1).set_visible(False)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c699b52f"
      },
      "source": [
        "### Histograms for Low Cardinality Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ec335f"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Calculate grid dimensions\n",
        "# num_plots = len(low_cardinality_cols)\n",
        "# num_rows = (num_plots + 3) // 4 # Equivalent to ceil(num_plots / 4)\n",
        "# num_cols = 4\n",
        "\n",
        "# plt.figure(figsize=(20, num_rows * 5)) # Adjust figure size for better readability\n",
        "\n",
        "# for i, col in enumerate(low_cardinality_cols):\n",
        "#     ax = plt.subplot(num_rows, num_cols, i + 1) # Create subplot\n",
        "#     sns.histplot(x_train[col], kde=True, ax=ax)\n",
        "#     ax.set_title(f'Histogram of {col}')\n",
        "#     ax.set_xlabel(col)\n",
        "#     ax.set_ylabel('Frequency')\n",
        "\n",
        "# # Hide any unused subplots if the last row is not full\n",
        "# for j in range(i + 1, num_rows * num_cols):\n",
        "#     plt.subplot(num_rows, num_cols, j + 1).set_visible(False)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64158d04"
      },
      "source": [
        "### Histograms for High Cardinality Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be70bbde"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Calculate grid dimensions\n",
        "# num_plots = len(high_cardinality_cols)\n",
        "# num_rows = (num_plots + 3) // 4 # Equivalent to ceil(num_plots / 4)\n",
        "# num_cols = 4\n",
        "\n",
        "# plt.figure(figsize=(20, num_rows * 5)) # Adjust figure size for better readability\n",
        "\n",
        "# for i, col in enumerate(high_cardinality_cols):\n",
        "#     ax = plt.subplot(num_rows, num_cols, i + 1) # Create subplot\n",
        "#     sns.histplot(train[col], kde=True, ax=ax)\n",
        "#     ax.set_title(f'Histogram of {col}')\n",
        "#     ax.set_xlabel(col)\n",
        "#     ax.set_ylabel('Frequency')\n",
        "\n",
        "# # Hide any unused subplots if the last row is not full\n",
        "# for j in range(i + 1, num_rows * num_cols):\n",
        "#     plt.subplot(num_rows, num_cols, j + 1).set_visible(False)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if there are any outliers\n",
        "\n",
        "outlier_data = []\n",
        "\n",
        "for col in continuous_cols:\n",
        "    lower_bound, upper_bound = show_outliers_iqr(train[col])\n",
        "    lower_outliers_count = (train[col] < lower_bound).sum()\n",
        "    upper_outliers_count = (train[col] > upper_bound).sum()\n",
        "    non_outliers_count = len(train[col]) - (lower_outliers_count + upper_outliers_count)\n",
        "\n",
        "    outlier_data.append({\n",
        "        'Column': col,\n",
        "        'Lower Bound Outlier %': f\"{lower_outliers_count/len(train[col]):.2f}\",\n",
        "        'Upper Bound Outlier %': f\"{upper_outliers_count/len(train[col]):.2f}\",\n",
        "        'Non-Outlier %': f\"{non_outliers_count/len(train[col]):.2f}\"\n",
        "    })\n",
        "\n",
        "# significant outliers\n",
        "outlier_df = pd.DataFrame(outlier_data)\n",
        "outlier_df_high = outlier_df[outlier_df['Upper Bound Outlier %'] > '0.20']\n",
        "outlier_df_low = outlier_df[outlier_df['Upper Bound Outlier %'] < '0.20']\n",
        "non_outlier_df = outlier_df[outlier_df['Upper Bound Outlier %'] == '0.00']\n",
        "# outlier_df"
      ],
      "metadata": {
        "id": "NeG912R_5ksV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('non_outliers/ outlier_high /outlier_low')\n",
        "print(non_outlier_df.shape, outlier_df_high.shape, outlier_df_low.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s6KylLKzhPr",
        "outputId": "357234c3-ddd8-41ed-d242-e4c2bc0ac1e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non_outliers/ outlier_high /outlier_low\n",
            "(3, 4) (1, 4) (25, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some columns have a large portion of outliers, will perform clipping rather than filtering\n",
        "train = replace_outliers_iqr(train, outlier_df_high['Column'].tolist())\n",
        "test = replace_outliers_iqr(test, outlier_df_high['Column'].tolist())\n",
        "\n",
        "print(\"Outliers columns capped in both train and test dataframes.\")\n",
        "risky_show_shape(train, test)"
      ],
      "metadata": {
        "id": "k5mwWUIXV8Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c67c06d-e3d8-4886-a227-332761c70ce7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers columns capped in both train and test dataframes.\n",
            "train/test\n",
            "(17534, 45) (8233, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = remove_outliers_iqr(train, outlier_df_low['Column'].tolist())\n",
        "test = remove_outliers_iqr(test, outlier_df_low['Column'].tolist())\n",
        "\n",
        "print(\"Outliers filtered in both train and test dataframes.\")\n",
        "risky_show_shape(train, test)"
      ],
      "metadata": {
        "id": "C9wTVL1kXQCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afdce68-cdc5-469a-f879-8f5a841e1b73"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers filtered in both train and test dataframes.\n",
            "train/test\n",
            "(6494, 45) (3292, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK2g0CB9Ssr8",
        "outputId": "e5dd317a-a52d-493e-bce4-25a3b15a48b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17534, 43)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter non_numerical_cols to only include those present in x_train\n",
        "for col in non_numerical_cols:\n",
        "    print(f\"Value counts for column '{col}':\\n{x_train[col].value_counts()}\\n\")"
      ],
      "metadata": {
        "id": "NGPEWwTLjwDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fca9b0e-d160-45fb-ae66-0d3b2ecdc535"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value counts for column 'proto':\n",
            "tcp           8009\n",
            "udp           6341\n",
            "unas          1224\n",
            "arp            303\n",
            "ospf           276\n",
            "sctp           103\n",
            "pim             31\n",
            "mobile          28\n",
            "any             27\n",
            "gre             26\n",
            "swipe           22\n",
            "ipv6            21\n",
            "scps            17\n",
            "sun-nd          17\n",
            "sps             17\n",
            "sep             16\n",
            "rsvp            16\n",
            "micp            16\n",
            "encap           15\n",
            "a/n             15\n",
            "pipe            15\n",
            "argus           15\n",
            "cbt             15\n",
            "mux             15\n",
            "rdp             14\n",
            "netblt          13\n",
            "crtp            13\n",
            "aes-sp3-d       13\n",
            "wb-mon          13\n",
            "leaf-1          12\n",
            "              ... \n",
            "ip               7\n",
            "iso-ip           7\n",
            "ptp              7\n",
            "bna              7\n",
            "ttp              7\n",
            "ggp              7\n",
            "zero             7\n",
            "irtp             7\n",
            "i-nlsp           7\n",
            "sat-expak        6\n",
            "smp              6\n",
            "snp              6\n",
            "wb-expak         6\n",
            "emcon            6\n",
            "kryptolan        6\n",
            "sat-mon          6\n",
            "narp             6\n",
            "mhrp             6\n",
            "ipcomp           6\n",
            "leaf-2           5\n",
            "chaos            5\n",
            "cphb             5\n",
            "nsfnet-igp       5\n",
            "3pc              4\n",
            "iatp             4\n",
            "eigrp            4\n",
            "mfe-nsp          4\n",
            "wsn              4\n",
            "sprite-rpc       4\n",
            "icmp             2\n",
            "Name: proto, Length: 131, dtype: int64\n",
            "\n",
            "Value counts for column 'service':\n",
            "-           9395\n",
            "dns         4719\n",
            "http        1887\n",
            "smtp         503\n",
            "ftp-data     411\n",
            "ftp          361\n",
            "ssh          118\n",
            "pop3         111\n",
            "dhcp          11\n",
            "snmp          10\n",
            "ssl            5\n",
            "radius         2\n",
            "irc            1\n",
            "Name: service, dtype: int64\n",
            "\n",
            "Value counts for column 'state':\n",
            "INT    8250\n",
            "FIN    7806\n",
            "CON    1265\n",
            "REQ     201\n",
            "RST      10\n",
            "ECO       2\n",
            "Name: state, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning values\n",
        "binned_x_train = x_train.copy()\n",
        "binned_x_test = x_test.copy()\n",
        "\n",
        "try:\n",
        "    for col in non_numerical_cols:\n",
        "        # Ensure we only try to bin columns that are actually features and present in non_float_cols\n",
        "        if col in non_numerical_cols: # Added check to prevent KeyError if non_float_cols doesn't have it\n",
        "            values = bin_decision(x_train[col])\n",
        "            binned_x_train = binning(binned_x_train, col, values)\n",
        "            binned_x_test = binning(binned_x_test, col, values)\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in non_float_cols for bin_decision. Skipping.\")\n",
        "    risky_show_shape(binned_x_train, binned_x_test)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"Error in binning\")"
      ],
      "metadata": {
        "id": "keg74cNm4f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cee9365-f271-4004-d7a8-15fd938e486b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/test\n",
            "(17534, 43) (8233, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Filter numerical_cols to only include those present in binned_x_train\n",
        "# columns_to_plot = [col for col in numerical_cols if col in binned_x_train.columns]\n",
        "\n",
        "# # Calculate grid dimensions\n",
        "# num_plots = len(columns_to_plot)\n",
        "# num_cols = 4\n",
        "# num_rows = (num_plots + num_cols - 1) // num_cols # Equivalent to ceil(num_plots / num_cols)\n",
        "\n",
        "# plt.figure(figsize=(20, num_rows * 5)) # Adjust figure size for better readability\n",
        "\n",
        "# for i, col in enumerate(columns_to_plot):\n",
        "#     ax = plt.subplot(num_rows, num_cols, i + 1) # Create subplot\n",
        "#     sns.histplot(binned_x_train[col], kde=True, ax=ax)\n",
        "#     ax.set_title(f'Histogram of {col}')\n",
        "#     ax.set_xlabel(col)\n",
        "#     ax.set_ylabel('Frequency')\n",
        "\n",
        "# # Hide any unused subplots if the last row is not full\n",
        "# for j in range(i + 1, num_rows * num_cols):\n",
        "#     plt.subplot(num_rows, num_cols, j + 1).set_visible(False)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "h-yXpgN9cY_g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3f2e58f",
        "outputId": "6dbe3b28-c868-4de7-8a3e-134c9e041df7"
      },
      "source": [
        "# one hot encoding\n",
        "\n",
        "# Combine train and test data for consistent one-hot encoding\n",
        "combined_x = pd.concat([binned_x_train, binned_x_test], ignore_index=True)\n",
        "\n",
        "encoded_combined_x = combined_x.copy()\n",
        "for col in non_numerical_cols:\n",
        "    encoded_combined_x = col_encoder(encoded_combined_x, col)\n",
        "\n",
        "# Split back into train and test\n",
        "encoded_x_train = encoded_combined_x.iloc[:len(binned_x_train)].copy()\n",
        "encoded_x_test = encoded_combined_x.iloc[len(binned_x_train):].copy()\n",
        "\n",
        "risky_show_shape(encoded_x_train,encoded_x_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/test\n",
            "(17534, 191) (8233, 191)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark lib"
      ],
      "metadata": {
        "id": "SZNLsYl-hqK5"
      }
    }
  ]
}